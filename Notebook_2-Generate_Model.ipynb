{"cells":[{"cell_type":"markdown","metadata":{"id":"WWw1-IytNY5K"},"source":["<table class=\"ee-notebook-buttons\" align=\"left\">\n","    <td><a target=\"_blank\"  href=\"https://github.com/davidelomeo/mangroves_deep_learning/blob/main/Notebook_2-Generate_Model.ipynb\"><img width=32px src=\"https://www.tensorflow.org/images/GitHub-Mark-32px.png\" /> View source on GitHub</a></td>\n","    <td><a target=\"_blank\"  href=\"https://colab.research.google.com/github/davidelomeo/mangroves_deep_learning/blob/main/Notebook_2-Generate_Model.ipynb\"><img src=\"https://www.tensorflow.org/images/colab_logo_32px.png\" /> Run in Google Colab</a></td>\n","</table>"]},{"cell_type":"markdown","metadata":{"id":"ISI2fzQZp07o"},"source":["# **Requirements**\n","The requirements to run this notebook are:\n","1. Have a @gmail account **->** here how to get one: https://support.google.com/accounts/answer/27441?hl=en\n","2. Have a Google Earth Engine account setup **->** here how to get one: https://signup.earthengine.google.com/\n","3. (Optional) Have a Google Coud Storage setup **->** here how to get one: https://cloud.google.com/storage *(please read Note 2 below)*\n","\n","---\n","**Note**: Google Earth Engine is a free to use online tool, but it requires authorisation from Google first. After signing up, it may take a few days before being able to access the platform.\n","\n","**Note 2**: Google Cloud Storage IS NOT a free tool. Like many other cloud services, it has different costs for different services. <br/>\n","At the time of writing of this notebook, Google offers new Cloud Storage users a 90-day free trial with some funds attached to it.<br/>\n","Please find more info at: https://cloud.google.com/free/docs/gcp-free-tier <br/>"]},{"cell_type":"markdown","metadata":{"id":"HccCSw6yp05F"},"source":["# **Objective**\n","\n","This Notebook has the purpose of loading pacthes as TFRecords from the target storage, split the data into training, test an optionally validation datasets, convert them into dataset readable by a Keras model and train the target model.\n","\n","The model is then saved to the target storage method for later use in Notebook 3 to make predictions."]},{"cell_type":"markdown","metadata":{"id":"wXMeA_m-p02B"},"source":["# 1. Preparing the workspace"]},{"cell_type":"markdown","metadata":{"id":"EO0iEFcQq4tn"},"source":["## Cloning the Github Repository\n","The github repository that stores the project is cloned to the workspace to allow accessing the needed packages."]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":1432,"status":"ok","timestamp":1627556364739,"user":{"displayName":"Davide Lomeo","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgWsZLbvtgezThoXj2b1tjFJ0wYO7a6DEWpVUWiVQ=s64","userId":"00476587349921876723"},"user_tz":-60},"id":"_YnsAHcrL5GQ","outputId":"9ababd98-4acc-4473-b858-ffd6966b777a"},"outputs":[],"source":["github_repo = \"https://github.com/davidelomeo/mangroves_deep_learning.git\"\n","print(\"Github Repository: \", github_repo)\n","\n","!git clone \"{github_repo}\" # clone the github repository"]},{"cell_type":"markdown","metadata":{"id":"mgoaObDVq8ag"},"source":["## Installing the required packages\n","Although Google Colab has a pre-installed environment that contains many packages, a `requirement.txt` was provided in the GitHub repository for consistency (please see disclaimer below).\n","\n","The following code also install custom packages created specifically to facilitate the reproducibility of some key parts of the worfkflow, and hence allow the user to re-use these packages in other projects.\n","\n","---\n","**Disclaimer**: The notebook was specifically designed to work on Google Colab. The user may use the notebook on a local machine (e.g. using jupyter notebook), but mounting the Google Drive will not be possible with the method showed below. In that scenario, the user may need to use Google Cloud Storagae only."]},{"cell_type":"code","execution_count":2,"metadata":{"executionInfo":{"elapsed":16097,"status":"ok","timestamp":1627556380833,"user":{"displayName":"Davide Lomeo","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgWsZLbvtgezThoXj2b1tjFJ0wYO7a6DEWpVUWiVQ=s64","userId":"00476587349921876723"},"user_tz":-60},"id":"rhccM8LgMGUT"},"outputs":[],"source":["# Installing requirements.txt\n","# '&> /dev/null' allows to hide the terminal output when running the command\n","!pip install -r mangroves_deep_learning/requirements.txt &> /dev/null"]},{"cell_type":"markdown","metadata":{"id":"CNQcJu9urENW"},"source":["## Importing the required packages\n","Here the code imports all the needed packages for this notebook.\n","\n","**Note**: it is necessary to authenticate Google Drive and Google Earth Engine to use the notebook. Make sure to have previoulsy created the necessary accounts. If te user wants to use the Google Cloud Storage, then this also need to be authenticated as below.\n","\n","---\n","As the cell below is executed, both Google Drive and Google Earth Engine will require authentication. Please select the links that will appear below the code - this will open a new tab in the browser - login with the desired gmail account, allow Google to access the application and copy the key that will show on screen inside the box below."]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":1156,"status":"ok","timestamp":1627556585239,"user":{"displayName":"Davide Lomeo","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgWsZLbvtgezThoXj2b1tjFJ0wYO7a6DEWpVUWiVQ=s64","userId":"00476587349921876723"},"user_tz":-60},"id":"WNy0dJwYMHyO","outputId":"97f727a9-a893-4723-e3ed-8b1257f3e3ca"},"outputs":[],"source":["import os\n","import ee\n","import geemap\n","import datetime\n","import json\n","import subprocess\n","import numpy as np\n","import tensorflow as tf\n","import tensorflow_addons as tfa\n","import matplotlib.pyplot as plt\n","\n","# These are custom packages. Please see the README in the repo for details.\n","import eeCustomDeepTools as cdt\n","import CustomNeuralNetworks as cnn \n","\n","from pprint import pprint\n","from pathlib import Path\n","\n","from tensorflow import keras\n","from tensorflow.keras import layers\n","from tensorflow.keras.models import Model\n","from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n","from tensorflow.python.tools import saved_model_utils\n","\n","from google.colab import auth, drive\n","# Authorising Google Colab notebook to access the target Google Drive and mount it\n","drive.mount('/content/drive')\n","\n","# Authorising Google Colab to access the Google Earth Engine account\n","try:\n","    ee.Initialize()\n","except Exception as e:\n","    ee.Authenticate()\n","    ee.Initialize()\n","\n","# outputting plots in the notebook\n","%pylab inline\n","#loading tensorboard notebook extensions\n","%load_ext tensorboard"]},{"cell_type":"markdown","metadata":{"id":"Jv8x1AKZrg21"},"source":["----> Only Run the next cell if wanting to export the generated models to Google Cloud Storage or loading a pre-trained model from Google Cloud Storage\n","\n","---\n","Please authenticate Google Cloud Storage as done above with Google Drive and Earth Engine"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"jw46zUc9reV3"},"outputs":[],"source":["# Authorising Google Colab notebook to access the target Google Cloud account\n","auth.authenticate_user()"]},{"cell_type":"markdown","metadata":{"id":"rda9-E6ArngK"},"source":["\n","## Checking GPU availability\n","Checking if a GPU is available. This task is useful especially if the user has a basc Google Colab account, for which the GPU availability it time-restricted.\n","\n","Having a GPU available is essential for training models. Without an available GPU, training data becomes unfeasable.\n","\n","If there is no available GPU, then please try to run this notebook later."]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":3651,"status":"ok","timestamp":1627556452459,"user":{"displayName":"Davide Lomeo","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgWsZLbvtgezThoXj2b1tjFJ0wYO7a6DEWpVUWiVQ=s64","userId":"00476587349921876723"},"user_tz":-60},"id":"7Ir-cnBrr0VS","outputId":"3922ea3f-4cf2-4f1b-c5ce-7f92feff0eb0"},"outputs":[],"source":["device_name = tf.test.gpu_device_name()\n","if device_name != '/device:GPU:0':\n","  raise SystemError('GPU device not found')\n","print('Found GPU at: {}'.format(device_name))"]},{"cell_type":"markdown","metadata":{"id":"ZjjaTJj7sMty"},"source":["## Loading the .json file with the info about the patches"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":489,"status":"ok","timestamp":1627571827321,"user":{"displayName":"Davide Lomeo","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgWsZLbvtgezThoXj2b1tjFJ0wYO7a6DEWpVUWiVQ=s64","userId":"00476587349921876723"},"user_tz":-60},"id":"haYcP78eYRpL","outputId":"b7a5b397-d7d0-4355-8aa0-74796b207e13"},"outputs":[],"source":["# loading the json file that contains the name and the path of the exported\n","# paches as saved in Notebook_1-Generate_Patches\n","exported_pacthes_info = '/content/drive/MyDrive/...FILENAME.js' # change this path accordingly\n","with open(exported_pacthes_info) as j:\n","  patches_info = json.load(j)\n","\n","pprint(patches_info)"]},{"cell_type":"markdown","metadata":{"id":"eiABBFXOuvIb"},"source":["## Setting global variables\n","The variables below are by default aken from the .json file saved at the end of Notebook 1. Please provide your own info you have not saved data using Notebook 1"]},{"cell_type":"code","execution_count":28,"metadata":{"executionInfo":{"elapsed":186,"status":"ok","timestamp":1627571829610,"user":{"displayName":"Davide Lomeo","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgWsZLbvtgezThoXj2b1tjFJ0wYO7a6DEWpVUWiVQ=s64","userId":"00476587349921876723"},"user_tz":-60},"id":"TjLXsVxmRnIT"},"outputs":[],"source":["patch_size = patches_info['pixels']\n","year = patches_info['year']\n","\n","pacthes_folder = patches_info['folder'] + '/'\n","patches_prefix = patches_info['prefix']\n","\n","classes_label = 'classes'\n","\n","classes = patches_info['classes']\n","n_classes = len(classes)\n","\n","bands = patches_info['bands']"]},{"cell_type":"markdown","metadata":{"id":"tuTZH6TARqWq"},"source":["----> Only use the following code if getting data from **Google Drive**\n","\n","---\n"]},{"cell_type":"code","execution_count":29,"metadata":{"executionInfo":{"elapsed":197,"status":"ok","timestamp":1627571832315,"user":{"displayName":"Davide Lomeo","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgWsZLbvtgezThoXj2b1tjFJ0wYO7a6DEWpVUWiVQ=s64","userId":"00476587349921876723"},"user_tz":-60},"id":"apbckryUMS0T"},"outputs":[],"source":["# Pointing to data in Google Drive\n","gdrive = Path('/content/drive/MyDrive/')\n","patches_path = gdrive / pacthes_folder"]},{"cell_type":"markdown","metadata":{"id":"F9VpM3YwSOWU"},"source":["----> Only use the following code if getting data from **Google Coud Storage**\n","\n","---\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"gEKQrmwgQxkz"},"outputs":[],"source":["# Name of the Google Cloud Storage Bucket: please change to your own\n","bucket = 'mangroves_classification_bucket'\n","\n","# Pointing to data in Google Cloud Storage\n","patches_path = !gsutil ls 'gs://'{bucket}'/'{pacthes_folder}'/'"]},{"cell_type":"markdown","metadata":{"id":"oUnjPcC9wGP4"},"source":["# 2. Import and prepare dataset\n","This section allows the user to import the dataset stored in the target Google storage, split the data and prepare the datasets for the Keras models"]},{"cell_type":"markdown","metadata":{"id":"T7T9ayrfwRts"},"source":["## Get TFRecords paths and info\n","Getting info about the exported patches. The folder where the patches were exported contains a mixer.json file that is automatically generated by Earth Engine."]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":490,"status":"ok","timestamp":1627571841669,"user":{"displayName":"Davide Lomeo","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgWsZLbvtgezThoXj2b1tjFJ0wYO7a6DEWpVUWiVQ=s64","userId":"00476587349921876723"},"user_tz":-60},"id":"ARcfaxbDIINv","outputId":"8cb24a06-f4df-4341-dde8-6ef9600f6344"},"outputs":[],"source":["# Loading the class to get the info. The class will look into Google Drive by default\n","info = cdt.GetFilesInfo()\n","records_list, json_file = info.get_files(patches_path, patches_prefix)\n","mixer = info.get_mixer(json_file)\n","\n","patch_nx = mixer['patchDimensions'][0]\n","patch_ny = mixer['patchDimensions'][1]\n","patches_tot = mixer['totalPatches']\n","patch_dims = [patch_nx, patch_ny]\n","pprint(mixer)"]},{"cell_type":"markdown","metadata":{"id":"tvT2ddR3wagk"},"source":["## Load TFRecords\n","The TFRecords filenames within the `list records_list` obtained above are passed to `tf.dat.TFRecordDataset` to generated a records dataset."]},{"cell_type":"code","execution_count":31,"metadata":{"executionInfo":{"elapsed":195,"status":"ok","timestamp":1627571844270,"user":{"displayName":"Davide Lomeo","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgWsZLbvtgezThoXj2b1tjFJ0wYO7a6DEWpVUWiVQ=s64","userId":"00476587349921876723"},"user_tz":-60},"id":"ygPQbH4GDEDg"},"outputs":[],"source":["# Create a dataset from the TFRecord file in Cloud Storage.\n","dataset = tf.data.TFRecordDataset(records_list, compression_type='GZIP')"]},{"cell_type":"markdown","metadata":{"id":"3fXp94k9w21s"},"source":["## Split data in Training, Test and optionally Validation datasets"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":197,"status":"ok","timestamp":1627571851586,"user":{"displayName":"Davide Lomeo","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgWsZLbvtgezThoXj2b1tjFJ0wYO7a6DEWpVUWiVQ=s64","userId":"00476587349921876723"},"user_tz":-60},"id":"LHzltSR5C90e","outputId":"6a8d72d3-3402-4f5c-cf8c-ba028fe37ceb"},"outputs":[],"source":["# Train-test split\n","training_chunk = 0.80\n","test_chunk = 0.10\n","valid_chunk = 0.10\n","\n","# splitting the dataset into training, validation and test\n","training_ds, test_ds, val_ds = cdt.dataset_split(dataset, patches_tot, training_chunk, test_chunk, valid_chunk)"]},{"cell_type":"markdown","metadata":{"id":"LS6iIg9SxEPO"},"source":["## Generate dictionary of features\n","Creating a dictionary of known size features is key for later mapping the patches and create multi-channel TensorFlow features"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":191,"status":"ok","timestamp":1627571858755,"user":{"displayName":"Davide Lomeo","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgWsZLbvtgezThoXj2b1tjFJ0wYO7a6DEWpVUWiVQ=s64","userId":"00476587349921876723"},"user_tz":-60},"id":"eVw7AG45n2ZI","outputId":"b52d4682-5375-46da-be5a-7c3cba10d755"},"outputs":[],"source":["bands_of_interest = bands[:12]\n","n_channels = len(bands_of_interest)\n","features_dict = cdt.get_features_dict(bands, classes_label, bands_of_interest, patch_dims)\n","pprint(features_dict)"]},{"cell_type":"markdown","metadata":{"id":"H0vmjSazxMIb"},"source":["## Prepare datasets for training in keras models"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":177,"status":"ok","timestamp":1627571862016,"user":{"displayName":"Davide Lomeo","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgWsZLbvtgezThoXj2b1tjFJ0wYO7a6DEWpVUWiVQ=s64","userId":"00476587349921876723"},"user_tz":-60},"id":"b9C8tU_wYC2Q","outputId":"08189cc7-ee89-4385-d4e6-94da1e7fdcdf"},"outputs":[],"source":["# Please modify these parameters as needed\n","training_batch_size = 12\n","test_batch_size = 1\n","\n","# Preparing the data to be fed to keras models\n","prepare_data = cdt.PrepareBatches(features_dict, n_classes, classes_label)\n","train_b, test_b, valid_b = prepare_data.prepare_batches(training_batch_size, test_batch_size, training_ds, test_ds, val_ds)\n","pprint(train_b)\n","pprint(test_b)\n","pprint(valid_b)"]},{"cell_type":"markdown","metadata":{"id":"x_ZHR9ZQxnvZ"},"source":["# 3. Training Keras models\n","In this section the dataset are fed to the target keras model for training"]},{"cell_type":"markdown","metadata":{"id":"JSUMnylDxsps"},"source":["## Load target model"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":608,"status":"ok","timestamp":1627489544419,"user":{"displayName":"Davide Lomeo","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgWsZLbvtgezThoXj2b1tjFJ0wYO7a6DEWpVUWiVQ=s64","userId":"00476587349921876723"},"user_tz":-60},"id":"44wBfr5BGLs1","outputId":"bab555a1-d517-468a-a295-2002cc30a858"},"outputs":[],"source":["u_net = cnn.UNet(n_classes)\n","image_shapes = tuple(patch_dims) + (n_channels,)\n","model = u_net.build_model(image_shapes)\n","model.summary()"]},{"cell_type":"markdown","metadata":{"id":"CG2eVBHfxwJ_"},"source":["## Compile the target model"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Jog0FRTnyqUr"},"outputs":[],"source":["# cannot use Accuracy when using the Sigmoid Focal cross entropy\n","model.compile(optimizer=tf.keras.optimizers.Adam(),\n","              loss=tfa.losses.SigmoidFocalCrossEntropy(),\n","              metrics=[tf.keras.metrics.Precision(name='prec'),\n","                       tf.keras.metrics.Recall(name='rec'),\n","                       tf.keras.metrics.CategoricalAccuracy(name='cat_acc'),\n","                       tf.keras.metrics.CategoricalCrossentropy(name='cat_xntrp'),\n","                       tf.keras.metrics.KLDivergence(name='KLDiv')])"]},{"cell_type":"markdown","metadata":{"id":"onfQxh3-VpoA"},"source":["## Identify target folder\n","Here a Google Drive path was selected by default, but the user may wish to change this to a Google Cloud Storage path"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":182,"status":"ok","timestamp":1627571876713,"user":{"displayName":"Davide Lomeo","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgWsZLbvtgezThoXj2b1tjFJ0wYO7a6DEWpVUWiVQ=s64","userId":"00476587349921876723"},"user_tz":-60},"id":"Ytnl7YW4_BPU","outputId":"0520f82b-de28-427d-991e-eb00219bf87d"},"outputs":[],"source":["folder_name = 'UNet_models'\n","model_subfolder_identifier = 'UNet_model_sig_foc_crossentropy'\n","path_to_model = str(gdrive) + '/' + folder_name + '/' + model_subfolder_identifier\n","print(path_to_model)"]},{"cell_type":"markdown","metadata":{"id":"yCt-Of2yUciP"},"source":["### Loading a pre-trained model (Optional)\n","Run the following cell only if wanting to load a pre-trained model to continue its training for more epochs"]},{"cell_type":"code","execution_count":16,"metadata":{"executionInfo":{"elapsed":3776,"status":"ok","timestamp":1627556557947,"user":{"displayName":"Davide Lomeo","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgWsZLbvtgezThoXj2b1tjFJ0wYO7a6DEWpVUWiVQ=s64","userId":"00476587349921876723"},"user_tz":-60},"id":"ISP4Wd2_6gW4"},"outputs":[],"source":["folder = '/content/drive/MyDrive/.../' # change path accordingly\n","model_name = 'MODEL_NAME.h5' # change name accordingly\n","model = keras.models.load_model(folder + model_name)"]},{"cell_type":"markdown","metadata":{"id":"zTg9zN7uVAX_"},"source":["## Setup model's callbacks and epochs\n","Here the user may decide to insert more model callbakcs. Callbakcs are used when training data to save the performance of the model at different stages of training.\n","\n"]},{"cell_type":"code","execution_count":36,"metadata":{"executionInfo":{"elapsed":481,"status":"ok","timestamp":1627571883548,"user":{"displayName":"Davide Lomeo","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgWsZLbvtgezThoXj2b1tjFJ0wYO7a6DEWpVUWiVQ=s64","userId":"00476587349921876723"},"user_tz":-60},"id":"Ee1G6kY6OUGX"},"outputs":[],"source":["# Setting numer of epochs\n","nepochs = 10\n","\n","# Setting up tensorboard folder to save model's logs\n","log_path = path_to_model + '/logs'\n","logdir = os.path.join(log_path, datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\"))\n","tb_callback = tf.keras.callbacks.TensorBoard(logdir, histogram_freq=1)\n","\n","# patience: Number of epochs with no improvement after which training will be stopped.\n","early_stopping = EarlyStopping(monitor='val_loss', patience=10)\n","\n","# Defining the path and the model name to save at each epoch\n","checkpoint_path =  path_to_model + '/epochs:{epoch:03d}.h5'\n","checkpoint = ModelCheckpoint(filepath=checkpoint_path)\n","\n","# Seting the pacth to the history of the model (where metrics are saved)\n","history_path = path_to_model + '/history.js'\n","\n","# val_acc. mode='max - val_loss, mode='min\n","# checkpoint = ModelCheckpoint(filepath=checkpoint_path, monitor='val_loss', save_best_only=False, mode='max')"]},{"cell_type":"markdown","metadata":{"id":"s-NP0-_gXHBW"},"source":["## Run the training\n","The code below trains the model with the input training and validation set.\n","\n","It also updates the automatically generated history dictionary by computing the f1 score and adding the total time taken by the model to run."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"CUX98oUlQmCq"},"outputs":[],"source":["# Starting the timing of the model runtime\n","start_time = time.time()\n","\n","# Fit the model to the training data.\n","history = model.fit(x = train_b, \n","                    epochs = nepochs, \n","                    validation_data = valid_b, \n","                    callbacks = [tb_callback,\n","                                 early_stopping,\n","                                 checkpoint])\n","\n","# Ending the timing of the model runtime\n","end_time = time.time()\n","total_time = end_time - start_time\n","print('\\nThe model has taken {:.3} minutes to run\\n'.format(total_time/60))\n","\n","# Rounding the values of the metrics for cleanliness\n","for k, v in history.history.items():\n","  history.history[k] = [round(i, 4) for i in v]\n","\n","# calculate F1_score for training and validation sets with the computed metrics\n","# (The F1 score can also be calculated automatically using tensorflow-addons.\n","# This metric, however, has demonstrated during this project to be heavy to\n","# compute and increased exponetnially the training time)\n","def F1(prec, rec):\n","  return 2 * (prec * rec) / (prec + rec)\n","\n","f1_score = []\n","val_f1_score = []\n","\n","for a, b, c, d, in zip (history.history['prec'], history.history['rec'], \n","                        history.history['val_prec'], history.history['val_rec']):\n","  f1_score.append(round(F1(a, b), 4))\n","  val_f1_score.append(round(F1(c, d), 4))\n","\n","# Adding the extra computed metrics to the history dictionary\n","history.history['f1_score'] = f1_score\n","history.history['val_f1_score'] = val_f1_score\n","history.history['time_taken'] = round(total_time/60, 3)\n","\n","# exporting the history discionary to the previously defined path\n","with open(history_path, 'w') as f:\n","    json.dump(history.history, f)"]},{"cell_type":"markdown","metadata":{"id":"kbR0EpgR0YS8"},"source":["### Explore tensorboard logs interactive plots (optional)\n","This line of code allows the user to plot the logs generated by the model in an interactive shell."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"azxANIt25ZcH"},"outputs":[],"source":["%tensorboard --logdir log_path"]},{"cell_type":"markdown","metadata":{"id":"WUTxyI5DbLLu"},"source":["## Evaluate the model\n","Here the model is evaluated using the previoulsy generated test dataset"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"NRc8yXpX1JfV"},"outputs":[],"source":["model.evaluate(x = test_b)"]},{"cell_type":"markdown","metadata":{"id":"0GijGei243qi"},"source":["### Plotting model metrics (Optional)\n","Loading the dictionary that contains the metrics from the target cloud storage and plot them"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":916},"executionInfo":{"elapsed":2374,"status":"ok","timestamp":1627489607295,"user":{"displayName":"Davide Lomeo","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgWsZLbvtgezThoXj2b1tjFJ0wYO7a6DEWpVUWiVQ=s64","userId":"00476587349921876723"},"user_tz":-60},"id":"F2SjAR_CzOSy","outputId":"af25cd77-bc14-4d0c-cae1-93e60a2ba9e9"},"outputs":[],"source":["# loading the json fil that contains the name and the path of the exported paches\n","exported_pacthes_info = '/content/drive/MyDrive/...FILENAME.js' #change this path accordingly\n","with open(exported_pacthes_info) as j:\n","  full_model_hist = json.load(j)\n","\n","fig, ax = plt.subplots(3, 2, figsize=(12, 12))\n","fig.tight_layout(w_pad=3, h_pad=4)\n","\n","# Plotting history for accuracy\n","ax[0, 0].plot(full_model_hist['cat_acc'])\n","ax[0, 0].plot(full_model_hist['val_cat_acc'])\n","ax[0, 0].set_title('Model Accuracy')\n","ax[0, 0].set_ylabel('accuracy')\n","ax[0, 0].set_xlabel('epoch')\n","ax[0, 0].legend(['train', 'validation'], loc='upper left')\n","\n","# Plotting history for categorical crossentropy\n","ax[0, 1].plot(full_model_hist['cat_xntrp'])\n","ax[0, 1].plot(full_model_hist['val_cat_xntrp'])\n","ax[0, 1].set_title('Model Crossentropy')\n","ax[0, 1].set_ylabel('categorical crossentropy')\n","ax[0, 1].set_xlabel('epoch')\n","ax[0, 1].legend(['train', 'validation'], loc='upper left')\n","\n","# Plotting history for loss\n","ax[2, 1].plot(full_model_hist['loss'])\n","ax[2, 1].plot(full_model_hist['val_loss'])\n","ax[2, 1].set_title('Model Loss')\n","ax[2, 1].set_ylabel('loss')\n","ax[2, 1].set_xlabel('epoch')\n","ax[2, 1].legend(['train', 'validation'], loc='upper left')\n","\n","# Plotting history for f1 score\n","ax[2, 0].plot(full_model_hist['f1_score'])\n","ax[2, 0].plot(full_model_hist['val_f1_score'])\n","ax[2, 0].set_title('Model F1 Score')\n","ax[2, 0].set_ylabel('f1 score')\n","ax[2, 0].set_xlabel('epoch')\n","ax[2, 0].legend(['train', 'validation'], loc='upper left')\n","\n","# Plotting history for precision\n","ax[1, 0].plot(full_model_hist['prec'])\n","ax[1, 0].plot(full_model_hist['val_prec'])\n","ax[1, 0].set_title('Model Precision')\n","ax[1, 0].set_ylabel('precision')\n","ax[1, 0].set_xlabel('epoch')\n","ax[1, 0].legend(['train', 'validation'], loc='upper left')\n","\n","# Plotting history for recall\n","ax[1, 1].plot(full_model_hist['rec'])\n","ax[1, 1].plot(full_model_hist['val_rec'])\n","ax[1, 1].set_title('Model Recall')\n","ax[1, 1].set_ylabel('recall')\n","ax[1, 1].set_xlabel('epoch')\n","ax[1, 1].legend(['train', 'validation'], loc='upper left')"]},{"cell_type":"markdown","metadata":{"id":"OkgLaYpqbsYS"},"source":["# 6. Access Notebook 3 to make predictions\n","<table class=\"ee-notebook-buttons\" align=\"left\">\n","    <td><a target=\"_blank\"  href=\"https://github.com/davidelomeo/mangroves_deep_learning/blob/main/Notebook_3-Make_Predictions.ipynb\"><img width=32px src=\"https://www.tensorflow.org/images/GitHub-Mark-32px.png\" /> Access Notebook_3 on Github</a></td>\n","    <td><a target=\"_blank\"  href=\"https://colab.research.google.com/github.com/davidelomeo/mangroves_deep_learning/blob/main/Notebook_3-Make_Predictions.ipynb\"><img src=\"https://www.tensorflow.org/images/colab_logo_32px.png\" /> Run Notebook_3 in Google Colab</a></td>\n","</table>"]}],"metadata":{"accelerator":"GPU","colab":{"authorship_tag":"ABX9TyO4y3DTU0LXNTRRUy3H0bpg","collapsed_sections":[],"name":"Notebook_2-Generate_Model.ipynb","provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}
